# -*- coding: utf-8 -*-
"""Cataract Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10FrM9Yv5J9XxGP_Fb26FPaYyJausxlrF

## Step 1: import files
"""

import cv2
print(cv2.__version__)

!unzip /content/dataset.zip

# Get filenames in list
from os import listdir
from os.path import isfile, join
mypath = "dataset/phase1/images"
file_names = [f for f in listdir(mypath) if isfile(join(mypath, f))]
print(str(len(file_names)) + ' images loaded')

"""## Step 2: Sift and Glcm demo on one example image"""

# Commented out IPython magic to ensure Python compatibility.
import cv2
import matplotlib.pyplot as plt
# %matplotlib inline
plt.rcParams["figure.figsize"] = (10,10)
#reading image
img1 = cv2.imread(mypath+"/"+file_names[21])
gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)

#keypoints
sift = cv2.xfeatures2d.SIFT_create()
keypoints_1, descriptors_1 = sift.detectAndCompute(gray1,None)

img_1 = cv2.drawKeypoints(gray1,keypoints_1,img1)
plt.imshow(img_1)
img1.shape
len(keypoints_1)

import numpy as np
import cv2
from skimage.feature import graycomatrix, graycoprops
from skimage.measure import shannon_entropy

# Load the image in grayscale
image_path = mypath + "/" + file_names[21]  # Make sure mypath and file_names are defined
image = cv2.imread(image_path, 0)

# Calculate and print Shannon entropy
img_arr = np.array(image)
entropy = shannon_entropy(img_arr)
print(f"Shannon Entropy: {entropy}")

# Compute the Gray Level Co-occurrence Matrix (GLCM)
gCoMat = graycomatrix(img_arr, [1], [0], 256, symmetric=True, normed=True)

# Extract GLCM properties
contrast = graycoprops(gCoMat, prop='contrast')[0][0]
dissimilarity = graycoprops(gCoMat, prop='dissimilarity')[0][0]
homogeneity = graycoprops(gCoMat, prop='homogeneity')[0][0]
energy = graycoprops(gCoMat, prop='energy')[0][0]
correlation = graycoprops(gCoMat, prop='correlation')[0][0]  # Corrected typo

# Print the extracted features
print("Contrast: ", contrast)
print("Dissimilarity: ", dissimilarity)
print("Homogeneity: ", homogeneity)
print("Energy: ", energy)
print("Correlation: ", correlation)

"""## Step 3: Sift and glcm on full dataset"""

import cv2
import numpy as np
from skimage.feature import  graycomatrix, graycoprops
images_sift = []
glcm=[]
labels = []
size = 128
sift = cv2.xfeatures2d.SIFT_create()
cataract=0
normal=0
for i, file in enumerate(file_names):
        image = cv2.imread(mypath+"/"+file,0)
        h,w=image.shape
        if(h>128 and w>128):
            image = cv2.resize(image, (size, size), interpolation = cv2.INTER_AREA)
            img_arr = np.array(image)
            gCoMat = graycomatrix(img_arr, [1], [0],256,symmetric=True, normed=True) # Co-occurance matrix
            contrast = graycoprops(gCoMat, prop='contrast')[0][0]
            dissimilarity = graycoprops(gCoMat, prop='dissimilarity')[0][0]
            homogeneity = graycoprops(gCoMat, prop='homogeneity')[0][0]
            energy = graycoprops(gCoMat, prop='energy')[0][0]
            correlation = graycoprops(gCoMat, prop='correlation')[0][0]
            keypoints, descriptors = sift.detectAndCompute(image,None)
            descriptors=np.array(descriptors)
            descriptors=descriptors.flatten()
            glcm.append([contrast,dissimilarity,homogeneity,energy,correlation])
            images_sift.append(descriptors[:2304])

            #print(descriptors.shape)
            if file_names[i][0] == "c":
                cataract+=1
                labels.append(1)
            if file_names[i][0] == "n":
                normal+=1
                labels.append(0)

print("Testing and validation split done!")

labels

images_sift=np.array(images_sift)
images_sift.shape

glcm=np.array(glcm)
images_sift_glcm=np.concatenate((images_sift,glcm),axis=1)
images_sift_glcm.shape

"""## Step 4: Building various  ML models"""

from sklearn import preprocessing
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


rf_class = RandomForestClassifier(n_estimators=100)
svm_rbf=svm.SVC(kernel='rbf',gamma=0.001,C=10)
svm_linear=svm.SVC(kernel='linear',gamma=0.001,C=10)
knn= KNeighborsClassifier(n_neighbors=2, metric='minkowski', p=2)
log = LogisticRegression(solver='liblinear')

model_names={"Random Forest":rf_class,"SVM RBF":svm_rbf,"SVM_linear":svm_linear,"k nearest neighbor":knn,"logistic regression":log}

"""## Step 5: Testing various model"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

def testing(model_name, X_train, X_test, y_train, y_test):
    # Retrieve the model based on the model name
    model = model_names[model_name]

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions on the test set
    yhat = model.predict(X_test)

    # Evaluate predictions
    acc = accuracy_score(y_test, yhat)
    precision = precision_score(y_test, yhat)
    recall = recall_score(y_test, yhat)
    f1 = f1_score(y_test, yhat)
    conf_matrix = confusion_matrix(y_test, yhat)

    # Print evaluation results
    print(f"{model_name} \tAccuracy: {acc:.3f}")
    print(f"{model_name} \tPrecision: {precision:.3f}")
    print(f"{model_name} \tRecall: {recall:.3f}")
    print(f"{model_name} \tF1-Score: {f1:.3f}")
    print(f"{model_name} \tConfusion Matrix:\n{conf_matrix}\n\n")

# Usage example
# testing("SVM", X_train, X_test, y_train, y_test)

def result(dataset):
    #Normalization
    #min_max_scaler = preprocessing.StandardScaler()
    #x_scaled = min_max_scaler.fit_transform(dataset)


    #panda dataframe
    df=pd.DataFrame(data=dataset)
    df['label']=labels
    df=df.sample(frac=1)
    X=df.drop(['label'], axis = 1)
    y=df['label']


    #Different model
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
    for model in ["Random Forest","SVM RBF","SVM_linear","k nearest neighbor","logistic regression"]:
        testing(model,X_train, X_test, y_train, y_test)

"""## Testing Sift features"""

print("Sift testing..........\n")
result(images_sift)

"""## GLCM features"""

print("glcm testing..........\n")
result(glcm)

"""## sift and glcm combined features"""

print("sift and glcm combined testing..........\n")
result(images_sift_glcm)

import pickle

# Assuming 'log_model' is your trained logistic regression model
# Replace 'log_model' with the name of your model variable if different

# Specify the filename for saving the model
model_filename = 'log_model.sav'

# Save the model to a file
with open(model_filename, 'wb') as model_file:
    pickle.dump(log, model_file)

print(f'Model saved to {model_filename}')

"""## Step 6:Predicting........."""

import pickle
import cv2
from skimage.feature import  graycomatrix, graycoprops
from sklearn import preprocessing
import numpy as np
log_pickle_model = pickle.load(open("log_model.sav", 'rb'))
sift = cv2.xfeatures2d.SIFT_create()
#min_max_scaler = preprocessing.MinMaxScaler()
size=128

def predict_new(image):
#     print(imagefile)
#     image_test = cv2.imread(mypath+"/"+imagefile,0)
    image_test = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    size = 128
    image_test = cv2.resize(image_test, (size, size), interpolation = cv2.INTER_AREA)
    glcm_test=[]
    images_sift_test=[]
    img_arr_test = np.array(image_test)
    gCoMat = graycomatrix(img_arr_test, [1], [0],256,symmetric=True, normed=True) # Co-occurance matrix
    contrast = graycoprops(gCoMat, prop='contrast')[0][0]
    dissimilarity = graycoprops(gCoMat, prop='dissimilarity')[0][0]
    homogeneity = graycoprops(gCoMat, prop='homogeneity')[0][0]
    energy = graycoprops(gCoMat, prop='energy')[0][0]
    correlation = graycoprops(gCoMat, prop='correlation')[0][0]
    keypoints, descriptors = sift.detectAndCompute(image_test,None)
    descriptors=np.array(descriptors)
    descriptors=descriptors.flatten()
    glcm_test.append([contrast,dissimilarity,homogeneity,energy,correlation])
    glcm_test=np.array(glcm_test)
    images_sift_test.append(descriptors[:2304])
    images_sift_test=np.array(images_sift_test)
    images_sift_glcm_test=np.concatenate((images_sift_test,glcm_test),axis=1)
#     if(imagefile[0]=='c'):
#         print("Actual: Cataract")
#     else:
#         print("Actual: Normal")
    if(loaded_model.predict(images_sift_glcm_test)==1):
        print("Predicted: Cataract")
    else:
        print("Predicted: Normal")
    print("******************************************************************")

for i in file_names:
    predict_new(i)

filename = 'log_model.sav'
log = LogisticRegression(solver='liblinear')
while(True):
    df=pd.DataFrame(data=images_sift_glcm)
    df['label']=labels
    df=df.sample(frac=1)
    X=df.drop(['label'], axis = 1)
    y=df['label']
    #Different model
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
    log.fit(X_train,y_train)
    yhat = log.predict(X_test)
    # evaluate predictions
    acc = accuracy_score(y_test, yhat)
    print(acc*100)
    if(acc>0.97):
        pickle.dump(log, open(filename, 'wb'))
        break

# load the model from disk
import pickle
filename = 'log_model.sav'
loaded_model = pickle.load(open(filename, 'rb'))

import numpy as np
import cv2
import matplotlib.pyplot as plt

# Load the classifiers
face_classifier = cv2.CascadeClassifier('/content/haarcascade_frontalface_default.xml')
eye_classifier = cv2.CascadeClassifier('/content/haarcascade_eye.xml')

# Load the image
img = cv2.imread('/content/test cataract.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Detect faces
faces = face_classifier.detectMultiScale(gray, 1.3, 5)

# Check if faces are found
if len(faces) == 0:
    print("No Face Found")
else:
    roi_eye = []  # List to store regions of interest for eyes
    for (x, y, w, h) in faces:
        # Draw a rectangle around the face
        cv2.rectangle(img, (x, y), (x+w, y+h), (127, 0, 255), 2)

        # Extract the region of interest (face)
        roi_gray = gray[y:y+h, x:x+w]
        roi_color = img[y:y+h, x:x+w]

        # Detect eyes within the face region
        eyes = eye_classifier.detectMultiScale(roi_gray)
        for (ex, ey, ew, eh) in eyes:
            # Extract the eye region of interest
            eye_region = roi_color[ey:ey+eh, ex:ex+ew]

            # Convert the eye region to a NumPy array and append to the list
            eye_region = np.array(eye_region)
            roi_eye.append(eye_region)

            # Draw a rectangle around the eyes
            cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (255, 255, 0), 2)

    # Convert the image from BGR to RGB for displaying in matplotlib
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Display the image with rectangles around the face and eyes
    plt.imshow(img_rgb)
    plt.axis('off')
    plt.show()

    # Check if roi_eye is a valid list of arrays
    if roi_eye:
        for i, eye in enumerate(roi_eye):
            print(f"Eye {i+1} is a valid NumPy array with shape: {eye.shape}")
    else:
        print("No eyes detected.")

import cv2
import matplotlib.pyplot as plt

# Assuming roi_eye contains the region of interest (eye) from your previous code
# roi_eye is a list of eye regions, so you can display each one

# Example: Display all detected eyes
for i, eye in enumerate(roi_eye):
    # Convert BGR (OpenCV format) to RGB (matplotlib format)
    eye_rgb = cv2.cvtColor(eye, cv2.COLOR_BGR2RGB)

    # Display using matplotlib
    plt.imshow(eye_rgb)
    plt.title(f'Eye {i+1}')
    plt.axis('off')
    plt.show()

predict_new(roi_eye[1])

